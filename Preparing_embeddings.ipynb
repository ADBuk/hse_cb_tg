{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6ee485",
   "metadata": {},
   "source": [
    "# Making embeddings from tg channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c4e2ce",
   "metadata": {},
   "source": [
    "### preparing  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee12c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f2f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw parsed texts data\n",
    "files = os.listdir()[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719951b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  9.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# uploading files to DataFrame\n",
    "data = pd.DataFrame()\n",
    "for i in tqdm(files):\n",
    "    data = pd.concat([data, pd.read_json(i)])\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18209430",
   "metadata": {},
   "outputs": [],
   "source": [
    "mes = data['messages'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8257f9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70687/70687 [00:00<00:00, 1570470.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# extracting texts\n",
    "texts = []\n",
    "for i in tqdm(mes):\n",
    "    try:\n",
    "        mes_str = i['text']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    if type(mes_str) == list:\n",
    "        if type(mes_str[0]) != dict:\n",
    "            if mes_str[0] != '':\n",
    "                texts.append(mes_str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3775970d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45545"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all texts len\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aed00286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting garbage chars\n",
    "for i in range(0, len(texts)):\n",
    "    texts[i] = texts[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d02289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving raw texts\n",
    "with open('texts_cb.pkl', 'wb') as f:\n",
    "    pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b4851",
   "metadata": {},
   "source": [
    "### bert encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27940430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model downloading\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")\n",
    "model = AutoModel.from_pretrained(\"sberbank-ai/sbert_large_nlu_ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7f7b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode function\n",
    "def encode(text:List):\n",
    "    encoding = tokenizer(text, return_tensors='pt', padding=True, max_length=512, truncation=True).to(device)\n",
    "    return model(**encoding).pooler_output.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a44d7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#countiong max_len of tg posts\n",
    "length = []\n",
    "\n",
    "for i in range(0, len(texts)):\n",
    "    length.append(len(texts[i].split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2124bceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_len\n",
    "np.array(length).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa3acc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model to cuda\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acfe6487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 24 00:26:21 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 537.13                 Driver Version: 537.13       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090      WDDM  | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   32C    P8              28W / 450W |  14717MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      4700      C   C:\\Users\\polte\\anaconda3\\python.exe       N/A      |\n",
      "|    0   N/A  N/A      7708    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8816    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      8944    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A      9200    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12672    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12808    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     14140    C+G   ...ndexBrowser\\Application\\browser.exe    N/A      |\n",
      "|    0   N/A  N/A     18344    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a53c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9109/9109 [06:39<00:00, 22.81it/s]\n"
     ]
    }
   ],
   "source": [
    "#computing embeddings\n",
    "X_all = np.zeros([1,1024])\n",
    "\n",
    "for i in tqdm(range(0, len(texts), 5)):\n",
    "    \n",
    "    X_all = np.append(X_all, encode(list(texts[i:i+5])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "697671ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deletting zeros array \n",
    "X_all = X_all[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b72a3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45545, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of embeddings\n",
    "X_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d917ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embeddings\n",
    "with open('texts_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(X_all, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
